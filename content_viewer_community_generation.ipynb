{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2f4c1e3242f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommunity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data are the relationships betweeen viewers and content. In this case the program content has been limited to program series and excluded movies, live news, special events, sports events, etc. \n",
    "\n",
    "The data has also been limited to on-demand viewing as opposed to live television viewing. This has been done because we want to focus on active content consumption (as opposed to 'channel surfing') because our larger goals are to model viwer and content interactions that we might see on streaming services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading our viewing data. This data has been engineered upstream and is a sample of a much larger dataset. Logically, it is a table of viewer-content engagement. Each row has one content (i.e. program) and one viewer. Because program to viewer relationships are many-to-many both programs and viewers will repeat within the dataset.\n",
    "\n",
    "_*Note that this dataset is a small sample of the actual dataset used so results here are not as representitive of the actual viewer / content universe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewing = pd.read_csv('./input/viewing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list all columns and show a subset of the data that illustrates what is in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewing[['personkey','age','gender','householdincome','contentsk','programname','engagement']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'personkey' and 'contentsk' are keys for viewers and programs respectively. 'engagement' is a refined feature that serves as a relative measure of how interested the viewer is with the program; based on what proportion of the content that the viewer consumes. For exmple, if the viewer watches every episode of Game of Thrones and watches 100% of the duration of those episodes, the engagement would be 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load the data into a network graph, so we need to do some data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes will be programs and viewers\n",
    "viewing['personkey'] = 'V' + viewing['personkey'].astype(str)\n",
    "viewing['contentsk'] = 'P' + viewing['contentsk'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a unique list of programs\n",
    "programs = viewing[['contentsk','programname','program_total_engagement']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the program node attributes. these attributes will be saved within the graph\n",
    "attribs = [None] * programs.shape[0]\n",
    "for i in range(0, programs.shape[0]):\n",
    "    attribs[i]={'type':'program', 'programname':programs.iloc[i]['programname'],'program_total_engagement':programs.iloc[i]['program_total_engagement']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now build the actual program node tuples that the graph requires that combine the keys and the attributes\n",
    "df_nodes = pd.DataFrame({'contentsk':programs['contentsk'],'attribs':attribs})\n",
    "node_tuples = [tuple(x) for x in df_nodes[['contentsk','attribs']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new empty graph and add the program nodes\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(node_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's prepare the viewer nodes\n",
    "viewer_cols = ['personkey','age', 'gender',\n",
    "       'race', 'person_education', 'person_education_level', 'countysize',\n",
    "       'county_size_level', 'householdincome', 'languageofhousehold',\n",
    "       'headofhoushold_education_level', 'householdsize', 'numberofchildren',\n",
    "       'numberofadults', 'numberofincomes', 'hascat', 'hasdog']\n",
    "\n",
    "# viewers\n",
    "viewers = viewing[viewer_cols].drop_duplicates()\n",
    "viewers['person_education'] = viewers.race.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the viewer node attributes\n",
    "attribs = [None] * viewers.shape[0]\n",
    "for i in range(0, viewers.shape[0]):\n",
    "    attribs[i]={'type':'viewer'\n",
    "    , 'age':viewers.iloc[i]['age']\n",
    "    , 'householdincome':viewers.iloc[i]['householdincome']\n",
    "    , 'numberofchildren':viewers.iloc[i]['numberofchildren']\n",
    "    , 'race':viewers.iloc[i]['race']\n",
    "    , 'countysize':viewers.iloc[i]['countysize']\n",
    "    , 'headofhoushold_education_level':viewers.iloc[i]['headofhoushold_education_level']\n",
    "    , 'person_education_level':viewers.iloc[i]['person_education_level']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now build the node tuples\n",
    "df_nodes = pd.DataFrame({'personkey':viewers['personkey'],'attribs':attribs})\n",
    "node_tuples = [tuple(x) for x in df_nodes[['personkey','attribs']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the viewer nodes to the graph\n",
    "G.add_nodes_from(node_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to define the edges, or the connections between the viewer nodes and the program nodes\n",
    "# we can save the engagement as an attribute of each edge\n",
    "links = viewing[['personkey','contentsk', 'engagement']].drop_duplicates()\n",
    "\n",
    "# like nodes, we add to the graph in bulk with tuples that look like this: (node1, node2, {'edge_attribute1': some_value, 'edge_attribute2': some_value})\n",
    "attribs = [None] * links.shape[0]\n",
    "for i in range(0, links.shape[0]):\n",
    "    attribs[i] = {'engagement':links.iloc[i]['engagement']}\n",
    "\n",
    "df_links = pd.DataFrame({'personkey':links['personkey'], 'contentsk':links['contentsk'], 'attribs':attribs})\n",
    "link_tuples = [tuple(x) for x in df_links[['personkey','contentsk','attribs']].values]\n",
    "\n",
    "# add the edges to the graph\n",
    "G.add_edges_from(link_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the graph summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have build the graph, we want to extract 'communities' from it to generate a rich taxonomy of viewer/content groups\n",
    "\n",
    "There are various community generation algorithms (see refences). Based on experimentation, we will use the Clauset-Newman-Moore greedy modularity maximization. This algorithm works well because within the on-demand viewing we find that the networks have a relatively high degree of modularity, where there are sparse connections between nodes in different groups. This reflects that fact that generally, viewers will like certain groups of programming and not like other types of programming at all.\n",
    "\n",
    "https://networkx.github.io/documentation/stable/reference/algorithms/community.html\n",
    "\n",
    "https://en.wikipedia.org/wiki/Community_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall approach will be the following:\n",
    "\n",
    "1. Extract the initial set of communities.  This will normally result in a small number of large communities and several smaller ones.\n",
    "\n",
    "2. Recursively treat each community as an isolated graph and contiune to generate sub-comminities within them.  \n",
    "\n",
    "This will generate a taxonomy of content and viewers similiar to biological taxonomies of species, music genres, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to define some functions that allow for recursion and setting attributes on each generated community 'group'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function pulls the info out of the community and creates a community group object with some statistics about it\n",
    "def build_community_group(group_list):\n",
    "    \n",
    "#    group_list = list(map(list, group_list))[0]\n",
    "    \n",
    "    contentsks = list(filter(lambda x: x.startswith('P'), group_list))\n",
    "    personkeys = list(filter(lambda x: x.startswith('V'), group_list))\n",
    "\n",
    "    group_viewing = viewing[viewing['contentsk'].isin(contentsks) & viewing['personkey'].isin(personkeys)]\n",
    "    \n",
    "    # group_viewing = viewing[viewing['contentsk'].isin(contentsks[0]) & viewing['personkey'].isin(personkeys[0])]\n",
    "    \n",
    "    n_contentsks= len(group_viewing['contentsk'].drop_duplicates())\n",
    "    n_personkeys = len(group_viewing['personkey'].drop_duplicates())\n",
    "    \n",
    "    group_viewers = viewing[viewing['personkey'].isin(personkeys)]\n",
    "    \n",
    "    pct_female = float(round(group_viewers[group_viewers['gender']=='F'].count()[0] / group_viewers.count()[0], 2))\n",
    "    pct_male = float(round(group_viewers[group_viewers['gender']=='M'].count()[0] / group_viewers.count()[0],2))\n",
    "    pct_children = float(round(group_viewers[group_viewers['numberofchildren']>0].count()[0] / group_viewers.count()[0],2))\n",
    "\n",
    "    # program_engagement = group_viewing.groupby('programname')['engagement'].sum().to_frame().reset_index().sort_values(by=['engagement'], ascending=False)\n",
    "    \n",
    "    # label the group by using the the top n programs based on combined ranking of engagement and number of viewers\n",
    "    program_engagement = group_viewing.groupby(['contentsk','programname'])['engagement'].sum().reset_index()\n",
    "    program_engagement['respondents'] = group_viewing.groupby(['contentsk','programname'])['personkey'].count().rank(ascending=False).tolist()\n",
    "    # program_engagement['combined_rank'] = program_engagement.engagement + program_engagement.respondents\n",
    "    # program_engagement = program_engagement.sort_values(by=['combined_rank'])\n",
    "    program_engagement = program_engagement.sort_values(by=['engagement'],ascending=False)\n",
    "    \n",
    "    group = {\n",
    "    'top_programs':\"|\".join(program_engagement['programname'][0:min(2,len(contentsks))])\n",
    "    , 'program_count':n_contentsks\n",
    "    , 'respondents':n_personkeys\n",
    "    , 'viewers_thousands':int(group_viewing.count()[0]/1000)\n",
    "    , 'avg_engagement':round(group_viewing['engagement'].mean(),0)\n",
    "    , 'median_engagement':round(group_viewing['engagement'].median(),0)\n",
    "    , 'median_age':group_viewers['age'].median()\n",
    "    , 'median_hh_income':group_viewers['householdincome'].median()\n",
    "    , 'pct_children':pct_children \n",
    "    , 'pct_female':pct_female\n",
    "    , 'pct_male':pct_male\n",
    "    , 'all_programs':'|'.join(program_engagement['programname']) \n",
    "    , 'contentsks':program_engagement['contentsk'].tolist()\n",
    "    , 'personkeys':personkeys\n",
    "    }\n",
    "    \n",
    "    return(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function allows for recusivity so that larger communites can be broken down to smaller ones \n",
    "# to build a hierarchy\n",
    "def build_community_groups(raw_communities, parent_group_id = \"\", group_level = 0):\n",
    "    \n",
    "    print(\"parent_group_id = {0}, len = {1}\".format(parent_group_id, len(raw_communities)))\n",
    "    \n",
    "    community_groups = []\n",
    "    i = 0\n",
    "    \n",
    "    for raw_community in raw_communities:\n",
    "        community_group = build_community_group(raw_community)\n",
    "        community_groups.append(community_group)\n",
    "        i = i + 1\n",
    "\n",
    "    community_groups = list(map(lambda x: build_community_group(x), raw_communities))\n",
    "    \n",
    "    df_community_groups = pd.DataFrame(community_groups, columns=['top_programs'\n",
    "                                                ,'program_count'\n",
    "                                                ,'respondents'\n",
    "                                                ,'viewers_thousands'\n",
    "                                                ,'avg_engagement'\n",
    "                                                ,'median_engagement'\n",
    "                                                ,'median_age'\n",
    "                                                ,'median_hh_income'\n",
    "                                                ,'pct_children'\n",
    "                                                ,'pct_female'\n",
    "                                                ,'pct_male'\n",
    "                                                , 'all_programs'\n",
    "                                                , 'contentsks'\n",
    "                                                , 'personkeys'\n",
    "                                                ])\n",
    "    \n",
    "    df_community_groups['group_id'] = [str(uuid.uuid4()).split('-')[0] for _ in range(len(df_community_groups.index))]\n",
    "    \n",
    "    df_community_groups['parent_group_id'] = parent_group_id\n",
    "    \n",
    "    df_community_groups['group_level'] = group_level\n",
    "    \n",
    "    # if the community has a certain amount of viewers and programs then continue to break it down\n",
    "    x = df_community_groups[(df_community_groups['respondents']>20) & (df_community_groups['program_count']>10)]\n",
    "    \n",
    "    sub_community_groups = list()\n",
    "    \n",
    "    if x.shape[0] > 0:\n",
    "      \n",
    "        for i in range(0, x.shape[0] - 1):\n",
    "            contentsks = x.loc[x.index[i], 'contentsks']\n",
    "            personkeys = x.loc[x.index[i], 'personkeys']\n",
    "            group_id = x.loc[x.index[i], 'group_id']\n",
    "            \n",
    "            subG = G.subgraph(contentsks + personkeys)  \n",
    "            sub_communities = community.greedy_modularity_communities(subG)\n",
    "            sub_community_groups.append(build_community_groups(sub_communities, parent_group_id = group_id, group_level = group_level + 1))  \n",
    "            \n",
    "        if len(sub_community_groups) > 0:\n",
    "            df_community_groups = df_community_groups.append(pd.concat(sub_community_groups))\n",
    "\n",
    "    \n",
    "    return(df_community_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's create our iniital top-level viewer/content communities \n",
    "# this can take a while depending on your local machine's resources\n",
    "initial_communities = community.greedy_modularity_communities(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's recurse through the communities and break them down to build a detailed taxonomy\n",
    "community_groups = build_community_groups(initial_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the smaller communities that have less than 6 viewers in them\n",
    "community_groups = community_groups[(community_groups['respondents']>5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peek at the table and look at the range of their respective attributes\n",
    "community_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_groups.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll save the following datasets that we will reloade back into our database so that we can join with other data to do more analysis on our viewing taxonomy; or the output files can be used for analysis in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_groups[['top_programs'\n",
    ",'program_count'\n",
    ",'respondents'\n",
    ",'viewers_thousands'\n",
    ",'avg_engagement'\n",
    ",'median_engagement'\n",
    ",'median_age'\n",
    ",'median_hh_income'\n",
    ",'pct_children'\n",
    ",'pct_female'\n",
    ",'pct_male'\n",
    ",'personkeys'\n",
    ",'contentsks'\n",
    ",'group_id'\n",
    ",'parent_group_id'\n",
    ",'group_level'\n",
    ",'all_programs']].to_csv(\"./output/viewing_communities.csv\", index = False, index_label = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewers.to_csv('./output/viewers.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programs.to_csv('./output/programs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
